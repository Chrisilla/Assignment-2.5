1) Explain what is a cluster and what is a hadoop cluster
	CLUSTER:
 	#A cluster refers to a group of machines that work together that perform a similar function.

	#Unlike grid computing, a computer cluster is controlled by a single software program that 
	manages all the computers or "nodes" within the cluster. 

	#The nodes work together to complete a single task. This process is called "parallel computing"
	since the nodes perform operations in tandem.

	HADOOP CLUSTER :
	*A computer cluster used for Hadoop is called Hadoop Cluster. 
	
	*Hadoop cluster increases the speed of data analysis applications.
 	
	*Hadoop cluster is designed specifically for storing and analyzing huge amounts of 
	unstructured data.
	*There are three types of nodes: 
		-master nodes
		-worker nodes
		-client nodes. 
	*Hadoop cluster will run in cheaper commodity hardwares.
	*Clusters involving of three or more machines use a single NameNode and ResourceManager
 	with the rest of the nodes as slave nodes.
	PROS:
	Hadoop clusters are  :
		#scalable
		#highly resistant to failures
		

2)Explain the components of Hadoop 1.x :


	-There are 2 components of hadoop :
	   #HDFS
	   #Map Reduce

	#HDFS

 		*HDFS stands for Hadoop Distributed File System.
 		*HDFS is developed using java.
 		*It  deals with a storage of large amounts of data in a reliable manner and also it helps to stream the data to user 
		 applications with high bandwidth.
 		*Hadoop uses HDFS for storage purpose.
 		*Hadoop Distributed File System take the data breaks them into pieces and distributes them to different nodes
  		in a cluster and thus allows parallel processing.
 		*HDFS is composed of two nodes. They are:
             		 -Namenode
              		-Datanode
 	*Namenode:
              		- The namenode is used to store metadata.
              		-It acts as the master node.
              		-It stores the information about the data nodes like
               		how many blocks are stored in the nodes,which node 
               		holds the data,node locations etc.
 	*Secondary namenode:
              		-secondary namenodes performs house-keeping activities 
              		 for name node like periodic merging and it is used for edits. 

 	*Datanode:
              		-The data nodes are used to store the actual data.
              		-It acts as slave nodes.
              		-The default size of the block is 64MB.
              		-It stores the data in terms of blocks
              		-The data node acts as the heart beat of the name node 
              		 as it periodically sends signals to the name node
              		 to check whether it is active.

	#Map Reduce

  		*Map reduce is also known as distributed data processing or batch processing model 
   		*It distributes the data across several machines.        

 		- Map reduce is composed of two components .They are:
                			-Job tracker.
                			-Task tracker. 
		*Job Tracker:
               			-The main role of job tracker is to assign the map reduce task to the task tracker.
               			-It is also used to re-assign some task to the other task tracker
                			when the previous task trackers fails or when it gets shut down.
               			-It is used to control the overall execution of map reduce.
               -			Job tracker also maintains the status of the task tracker.

		*Task Tracker:
               			-The task tracker sends the status back to the job tracker              
               			-The role of the task tracker is to execute the task assigned to it by the job tracker.
               			-It periodically communicates with the task tracker.
               			-It runs individual map-reduce tasks on the data nodes.


